{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     # Llibreria matemÃƒ tica\n",
    "import matplotlib.pyplot as plt        # Per mostrar plots\n",
    "import sklearn                         # Llibreia de DM\n",
    "import sklearn.datasets as ds            # Per carregar mÃƒÂ©s facilment el dataset digits\n",
    "import sklearn.model_selection as cv    # Pel Cross-validation\n",
    "import sklearn.neighbors as nb           # Per fer servir el knn\n",
    "%matplotlib inline              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>...</th>\n",
       "      <th>evidence</th>\n",
       "      <th>great.</th>\n",
       "      <th>toe</th>\n",
       "      <th>favor</th>\n",
       "      <th>soccer</th>\n",
       "      <th>did.</th>\n",
       "      <th>wheel</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>rank</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "0           0  815719226    False   finalized                   3   \n",
       "1           1  815719227    False   finalized                   3   \n",
       "2           2  815719228    False   finalized                   3   \n",
       "3           3  815719229    False   finalized                   3   \n",
       "4           4  815719230    False   finalized                   3   \n",
       "\n",
       "  _last_judgment_at  gender  gender:confidence profile_yn  \\\n",
       "0    10/26/15 23:24    male             1.0000        yes   \n",
       "1    10/26/15 23:30    male             1.0000        yes   \n",
       "2    10/26/15 23:33    male             0.6625        yes   \n",
       "3    10/26/15 23:10    male             1.0000        yes   \n",
       "4     10/27/15 1:15  female             1.0000        yes   \n",
       "\n",
       "   profile_yn:confidence   ...    evidence great.  toe favor soccer did.  \\\n",
       "0                    1.0   ...           0      0    0     0      0    0   \n",
       "1                    1.0   ...           0      0    0     0      0    0   \n",
       "2                    1.0   ...           0      0    0     0      0    0   \n",
       "3                    1.0   ...           0      0    0     0      0    0   \n",
       "4                    1.0   ...           0      0    0     0      0    0   \n",
       "\n",
       "  wheel shoulder  rank expected  \n",
       "0     0        0     0        0  \n",
       "1     0        0     0        0  \n",
       "2     0        0     0        0  \n",
       "3     0        0     0        0  \n",
       "4     0        0     0        0  \n",
       "\n",
       "[5 rows x 2027 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np    # Numeric and matrix computation\n",
    "import pandas as pd   # Optional: good package for manipulating data \n",
    "import sklearn as sk  # Package with learning algorithms implemented\n",
    "\n",
    "# Loading the dataset.\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\"\n",
    "data = pd.read_csv(\"dataWithTextWordsPrep.csv\", encoding='latin1', low_memory=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n",
      "['12/5/13 1:48' '10/1/12 13:51' '11/28/14 11:30' ..., '4/6/09 16:54'\n",
      " '11/18/10 11:16' '3/24/09 20:52']\n",
      "tweet_created\n",
      "['10/26/15 12:40' '10/26/15 12:39' '10/26/15 13:20' '10/26/15 13:19'\n",
      " '10/26/15 13:18']\n",
      "link_color\n",
      "['08C2C2' '0084B4' 'ABB8C2' ..., 'F7206F' '664422' '05B8CC']\n",
      "sidebar_color\n",
      "['FFFFFF' 'C0DEED' '0' '181A1E' 'EEEEEE' '65B0DA' 'EEA525' 'DBE9ED'\n",
      " 'BDDCAD' 'C6E2EE' 'F2E195' 'DFDFDF' '80808' 'D9B17E' '5ED4DC' 'FFF8AD'\n",
      " '87BC44' 'E59B38' '829D5E' 'D10707' 'CC3366' '0A0A0A' '515' '30303'\n",
      " 'A8C7F7' '035FFF' '22261' 'E5FFA6' 'ADF1FC' '69E2E8' 'F8F132' 'D3D2CF'\n",
      " 'FCFBD2' '2280A9' '50505' 'F8C7DA' '64533E' '827D82' '6B0237' 'A38CA3'\n",
      " 'F7F2F4' '487878' '151F24' 'AD1F74' 'FFDE00' '79B6E7' '0AB0F1' '12020F'\n",
      " '86A4A6' '3FC5A4' '2E0C2E' 'C0DCF1' '3BE0F4' '0066FF' '8CBAE2' '08A7F7'\n",
      " '7C6A87' '8BB65D' '89B5A2' 'CCCCCC' '6A6B69' '0E4766' 'D6BD89' 'EB63EB'\n",
      " 'BA7474' '0D0C0C' 'FAA005' '16818E' 'F50C0C' 'A310A3' 'FF0D3D' '6056'\n",
      " 'F8545F' 'FF08FF' 'B5B5B5' '141414' '292929' '4BB7DF' 'DADADA' 'FCF3B8'\n",
      " '999999' 'FFCE0A' '42444E' '212535' 'D179CB' '5C0F5C' 'E6D56A' 'ED2B45'\n",
      " '5C005C' '1D1D1D' 'FF5520' '80434B' 'E83407' '10508' 'C8A867' '404040'\n",
      " '00FF00' '06191A' '0D0508' 'F8858C' '10140E' 'FFB93C' 'C6B193' '5013C2'\n",
      " '09EDC7' 'E6E3E6' '4B0202' '2B1803' '30405' 'C2060C' 'F5F0F5' '12B2A6'\n",
      " 'EDE8C4' '6847' '0A090A' '0D0D09' '210E2A' 'AAB1B5' '4A66C1' '510094'\n",
      " '334A70' 'D8D8D8' '99E3F7' '142B45' 'FFFAFA' 'A7E74B' '463A4A' 'F00CF0'\n",
      " 'D1D1D1' '8B92C0' '1A1A1A' 'F50A29' '545151' 'D8C0A8' '3965B8' '032B05'\n",
      " '454B52' '47002B' '8EAA45' 'CDC6B3' '4D3445' '2AA14F' 'BD8700' '3D3D42'\n",
      " 'C2C2C2' '8D9290' 'E7796A' 'AEF5FA' '948C75' 'B8B8B8' '373737' '32436E'\n",
      " 'EE3A45' 'CAC8B1' 'DCE0E0' '1.10E+17' '38310B' 'F0E36E' 'FAFAFA' 'DA65B7'\n",
      " 'BCB302' 'DBADAD' '8A0000' 'FFBE3B' '0B486B' '4456BC' '821608' 'CBCBA5'\n",
      " '9A17EB' 'E7FF33' '1AAEED' '2665AD' 'CBCBCB' 'BDADDC' 'DCE4E0' 'FF5703'\n",
      " '0F0E0F' '3E38F2' '050C26' 'FFBF00' '0D0D0C' 'F0689E' '7A5C45' '333333'\n",
      " '085AF2' 'DAE5CD' '909090' '3FCDD9' 'BA223B' '333B36' 'B2B0BD' 'F7F7F7'\n",
      " 'F27908' '339103' 'E81288' '0C0F0F' '294654' '001B5E' '797EB4' 'C10020'\n",
      " 'FCF17C' 'B86307' '252429' 'CF4C28' '656869' '3E0405' '70808' '870000'\n",
      " '464646' '28426E' '050F00' '6.90E+29' '8BA7C4' 'EDF2F3' 'F4F4F4' '84FF42'\n",
      " 'BAB476' '2058B2' 'DB5ECC' 'ADB9DC' '1F5CD6' '0099FF' 'FAF7FA' 'F0E661'\n",
      " '3E3844' 'F51339' '9E139E' '07117D' 'D9DAE0' 'AB964A' '0D0014' 'E2EAEF'\n",
      " 'CBF7C1' 'F0A830' 'DB0F6B' 'F9FAFC' '204207' '935273' 'E30D23' 'DDDDDD'\n",
      " '2E3033' '76C8CF' '1F2403' 'F51714' '4878' '022C74' '69A555' '4C7AB4'\n",
      " 'F29008' '2A3A22' '003E6A' 'F6F7E3' 'E6056A' '0C0D0D' '9FE30D' 'E408F0'\n",
      " 'BFBFBF' '5E639C' '7FBFC7' '853385' 'F4F623' 'F08DF0' 'FFD8D6' '15192A'\n",
      " '4D4A33' 'E84F3A' '9C4B4C' '665C5C' '04131A' 'FF0D00' '264480' 'FF00E1'\n",
      " '00090A' '7F6262' '892DD8' '1A151A' '17191A' 'BF0606' '601B2F' '8CDE97'\n",
      " '8F0000' 'FF5454' '241344' '05D5FF' '1BDED4' '5314D7' 'FF0000' '1E1F1C'\n",
      " '1351CF' '161C1A' '50005' '68FA04' 'F0B00E' '1C222B' '256169' '8F5353'\n",
      " 'BDBAAE' 'D5C788' 'C9B180' 'FCFCFC' '737373' 'E8B5BD' '92AFB7' 'EDE6FA'\n",
      " 'F5E7F5' 'F51313' 'F2B56A' 'C4C4C4' '20455D' '7FBDCA' 'FF4878' 'ADA6A7'\n",
      " '08521D' '13F0F0' 'FCF4F4' '329996' 'FBFF00' '191A18' '570B0B' '6666'\n",
      " '4.43E+03' '56A0DE' '910510' '216587' '2C093C' 'CDCDCD' 'D0F2DD' '8F6635'\n",
      " '9E2828' 'F8BB84' '8CB2BD' 'D95E5E' '7A7A7A' 'DEDEDE' '800606' 'C22525'\n",
      " 'D6DBAD' '3624DE' 'DAF71E' '280D0D' '505' '6A6E75' 'FEF9E6' 'D2DADA'\n",
      " '40505' 'F74F60' '37AF3B' 'DEDADE' '635F53' '57444D' 'DBDBDB' '3366'\n",
      " '2.62E+55' '00FFFF' 'C44D58' '142829' 'F068C0' '5C0000' '363636' '898F68'\n",
      " '006AFF' 'D58861' '0A6E46' 'F7B565' '422702' '070A09' 'B1784B' 'FF9800'\n",
      " '50405' 'D49F0C' '0A0909' 'E8C40E' '833409' '412A32' 'E3E3E3' 'B4B070'\n",
      " 'F59733' 'FFA6A6' '80705' '79D4BD' 'F4F3F2' 'E60E18' '3C9434' '795C9C'\n",
      " '6B2679' '660000' '6CA63C' 'E00000' 'DCE2E4' 'FF6A00' '4C3E29' '1848A8'\n",
      " 'D11991' '323232' 'B47741' '097CE0' '5C5C5C' '305' '222222' 'E5E6CA'\n",
      " '316932' 'DB9A0F' '002D7A' 'F6F3EA' 'FCD3BB' 'A8851C' 'EA03FF' '6B2525'\n",
      " '9000FF' '210B21' '948858' '242622' 'E8E8E8' 'F51182' '44EBA8' 'A1A1A1'\n",
      " '5B3C19' '3.24E+36' '398757' '0FBF9F' '181830' '53777A' '93BB4B' 'B6AFA5'\n",
      " 'D51CFF' 'B124C7' 'FAD503' '7E8CD2' '828282' 'FF00FF' 'BD6738' '101112'\n",
      " '8F1344' 'FA84D5' '4808AE' 'A69B73' 'F8C0D5' 'DE2C56' 'F0F0F0' '186C90'\n",
      " 'B503A9' 'E3DB3F' 'A9CBF5' '696969' 'F511B8' 'DE741D' '005A87' '171A1C'\n",
      " 'D4D055' '8BD5A1' 'CCAF7B' 'CFBEB4' '9944FF' 'DE3E9E' 'FE2048' '572A57'\n",
      " '111212' '11F2CD' 'B09797' '401A40' '787878' 'F0F1F5' '6D2446' 'E62E0E'\n",
      " '00A897' 'FFF1E8' '0A020A' 'FDF1E1' 'C0001D' '9B9BA3' '20500' '80'\n",
      " '1D1E1F' 'B46E62' 'F7F2F2' 'EA3606' 'BEDFB6' 'FAF5FA' 'FF9900' '80000'\n",
      " '0FAFFF' 'F55F08' 'BAB1BE' '5.94E+61' '516C67' '202A2B' 'AEB83D' 'E0AC8A'\n",
      " 'B1C4E6' '513930' 'DB15D4' '73013C' '7C0404' '674669' '782B49' '252C09'\n",
      " 'AC77C7' 'FF1919' '93F5C9' '0D0106' '0E7E7E' '97B8AB' 'F8956C' '457656'\n",
      " '501' 'EBEBEB' 'F8F8E0' 'D14B4B' '33CCFF' 'E61224' '9B7CB6' '1E3947'\n",
      " '00F2E6' '21EB89' '2E2D27' '3E4AFF' 'B3757F' 'CCCECC' 'FA0B03' '20318'\n",
      " 'DEF5FE' 'E1F2FA' 'F5E108' 'F0DFB8' 'ADC6DB' '4198DB' '247B37' 'EC5D94'\n",
      " '858585' '090A0A' '464244' 'E82221' '520B0B' '5.22E+22' '92C7EA' '5EFF00'\n",
      " '0B0203' 'EBAAAA' '7A4D1C' '590025' 'E6E6E6' '40405' '46373F' 'A7737D'\n",
      " 'B68B9E']\n",
      "tweet_location\n",
      "['main; @Kan1shk3' nan 'clcncl' ..., 'Youngstown, OH' 'the 6ix'\n",
      " 'Glendale CA']\n",
      "user_timezone\n",
      "['Chennai' 'Eastern Time (US & Canada)' 'Belgrade'\n",
      " 'Pacific Time (US & Canada)' nan 'Central Time (US & Canada)' 'Amsterdam'\n",
      " 'Atlantic Time (Canada)' 'Arizona' 'London' 'Mountain Time (US & Canada)'\n",
      " 'Hawaii' 'Caracas' 'Krasnoyarsk' 'Casablanca' 'Tijuana' 'Abu Dhabi'\n",
      " 'America/Los_Angeles' 'Baghdad' 'Edinburgh' 'Tokyo' 'Riyadh' 'Hong Kong'\n",
      " 'Seoul' 'Athens' 'Dublin' 'Madrid' 'Quito' 'Brussels' 'Sydney' 'Berlin'\n",
      " 'Pretoria' 'Wellington' 'Beijing' 'Greenland' 'Urumqi' 'Alaska' 'Bangkok'\n",
      " 'Skopje' 'Melbourne' 'Monterrey' 'Jakarta' 'Rome' 'Europe/Athens' 'Paris'\n",
      " 'Cape Verde Is.' 'America/New_York' 'Yerevan' 'Nairobi' \"Nuku'alofa\"\n",
      " 'Mid-Atlantic' 'Saskatchewan' 'UTC' 'Bogota' 'Prague' 'Auckland'\n",
      " 'Bucharest' 'Brasilia' 'Irkutsk' 'Midway Island' 'America/Chicago'\n",
      " 'West Central Africa' 'Vienna' 'Ljubljana' 'Brisbane' 'Buenos Aires'\n",
      " 'Stockholm' 'Samoa' 'Santiago' 'Jerusalem' 'Mexico City' 'Newfoundland'\n",
      " 'International Date Line West' 'Helsinki' 'Volgograd' 'La Paz' 'Sofia'\n",
      " 'Sri Jayawardenepura' 'Zagreb' 'Kyiv' 'Lisbon' 'Karachi' 'Chihuahua'\n",
      " 'Mumbai' 'Cairo' 'Bern' 'New Delhi' 'Azores' 'Adelaide' 'Singapore'\n",
      " 'Tehran' 'Dhaka' 'Kuala Lumpur' 'Hanoi' 'America/Toronto' 'Almaty'\n",
      " 'Muscat' 'Asia/Karachi' 'Budapest' 'Solomon Is.' 'Indiana (East)'\n",
      " 'Bratislava' 'Africa/Cairo' 'Perth' 'Istanbul' 'Baku' 'America/Vancouver'\n",
      " 'New Caledonia' 'Europe/London' 'Kuwait' 'Minsk' 'Africa/Lagos' 'Mazatlan'\n",
      " 'Harare' 'Islamabad' 'Ekaterinburg' 'Tallinn' 'Copenhagen' 'Moscow'\n",
      " 'Warsaw' 'Georgetown' 'Yakutsk' 'Monrovia' 'EDT' 'GMT+3' 'Magadan'\n",
      " 'Tashkent' 'Central America' 'Fiji' 'Kabul' 'Canberra' 'Kolkata' 'Darwin'\n",
      " 'Ulaan Bataar' 'America/Boise' 'PDT' 'Osaka' 'CST' 'Europe/Paris' 'Riga'\n",
      " 'Vilnius' 'Lima' 'Guam' 'Hobart' 'BST' 'Africa/Nairobi' 'America/Detroit'\n",
      " 'CDT' 'Europe/Sarajevo' 'PST' 'Taipei' 'IST' 'Novosibirsk'\n",
      " 'America/Argentina/Buenos_Aires']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a1b110edeefa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mboolean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboolean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mboolean_to_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweet_coord'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_coord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a1b110edeefa>\u001b[0m in \u001b[0;36mboolean_to_binary\u001b[1;34m(column_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mboolean_to_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mboolean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboolean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mboolean_to_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweet_coord'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carlos\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2508\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2509\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2510\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a1b110edeefa>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(boolean)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mboolean_to_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mboolean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboolean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mboolean_to_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweet_coord'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "data = data.drop(['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at',\n",
    "               'gender:confidence', 'profile_yn', 'profile_yn:confidence', 'created', 'description',\n",
    "               'fav_number', 'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage',\n",
    "               'sidebar_color', 'text', 'tweet_count', 'tweet_created', 'tweet_id', 'tweet_location',\n",
    "               'retweet_count', 'tweet_coord', 'user_timezone'], axis=1);\n",
    "data = data[twigen.gender.notnull()]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation: split data into training and test sets (test 30% of data)\n",
    "(X_train, X_test,  y_train, y_test) = cv.train_test_split(datadt, labels, test_size=.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the stratification according to labels *y* that we demand in the spliting of data. The ionosphere dataset is small and with strarification we ensure we obtain the same proportion of examples of each class in training and test sets.\n",
    "\n",
    "**Remember**. Data should be numerical and normalized or standarized before using an SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is not strictily necessary in our dataset because almost all columns are in range -1..1 (except columns 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>...</th>\n",
       "      <th>user_timezone_Urumqi</th>\n",
       "      <th>user_timezone_Vienna</th>\n",
       "      <th>user_timezone_Vilnius</th>\n",
       "      <th>user_timezone_Volgograd</th>\n",
       "      <th>user_timezone_Warsaw</th>\n",
       "      <th>user_timezone_Wellington</th>\n",
       "      <th>user_timezone_West Central Africa</th>\n",
       "      <th>user_timezone_Yakutsk</th>\n",
       "      <th>user_timezone_Yerevan</th>\n",
       "      <th>user_timezone_Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "0           0  815719226    False   finalized                   3   \n",
       "1           1  815719227    False   finalized                   3   \n",
       "2           2  815719228    False   finalized                   3   \n",
       "3           3  815719229    False   finalized                   3   \n",
       "4           4  815719230    False   finalized                   3   \n",
       "\n",
       "  _last_judgment_at  gender  gender:confidence profile_yn  \\\n",
       "0    10/26/15 23:24    male             1.0000        yes   \n",
       "1    10/26/15 23:30    male             1.0000        yes   \n",
       "2    10/26/15 23:33    male             0.6625        yes   \n",
       "3    10/26/15 23:10    male             1.0000        yes   \n",
       "4     10/27/15 1:15  female             1.0000        yes   \n",
       "\n",
       "   profile_yn:confidence          ...          user_timezone_Urumqi  \\\n",
       "0                    1.0          ...                             0   \n",
       "1                    1.0          ...                             0   \n",
       "2                    1.0          ...                             0   \n",
       "3                    1.0          ...                             0   \n",
       "4                    1.0          ...                             0   \n",
       "\n",
       "   user_timezone_Vienna user_timezone_Vilnius user_timezone_Volgograd  \\\n",
       "0                     0                     0                       0   \n",
       "1                     0                     0                       0   \n",
       "2                     0                     0                       0   \n",
       "3                     0                     0                       0   \n",
       "4                     0                     0                       0   \n",
       "\n",
       "  user_timezone_Warsaw user_timezone_Wellington  \\\n",
       "0                    0                        0   \n",
       "1                    0                        0   \n",
       "2                    0                        0   \n",
       "3                    0                        0   \n",
       "4                    0                        0   \n",
       "\n",
       "   user_timezone_West Central Africa user_timezone_Yakutsk  \\\n",
       "0                                  0                     0   \n",
       "1                                  0                     0   \n",
       "2                                  0                     0   \n",
       "3                                  0                     0   \n",
       "4                                  0                     0   \n",
       "\n",
       "  user_timezone_Yerevan  user_timezone_Zagreb  \n",
       "0                     0                     0  \n",
       "1                     0                     0  \n",
       "2                     0                     0  \n",
       "3                     0                     0  \n",
       "4                     0                     0  \n",
       "\n",
       "[5 rows x 30757 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's see how to do that properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4691469288a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#scaler = StandardScaler().fit(X_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Apply the normalization trained in training data in both training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1)).fit(X_train)\n",
    "\n",
    "# Apply the normalization trained in training data in both training and test sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "Let's try an SVM with default parameters. Linear means that we are not using any kernel to move the data to a higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7adb65f200ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#knc = LinearSVC()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mknc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mknc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mknc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion matrix on test set:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#knc = LinearSVC() \n",
    "knc = SVC(kernel='linear')\n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad results. However, the linear SVM has parameter C that has to be adjusted. We will use *GridSearch* method to find the optimal value of C like we did in a previous notebook with the k value of the KNN algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of C values to test. We usualy test diverse orders of magnitude\n",
    "#Cs = np.logspace(-3, 11, num=15, base=10.0)\n",
    "Cs = np.logspace(-3, 5, num=9, base=10.0)\n",
    "\n",
    "param_grid = {'C': Cs}\n",
    "#grid_search = GridSearchCV(LinearSVC(), param_grid, cv=10)\n",
    "grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=10)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Let's plot the 10-fold cross.validation accuracy deppending on C\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plt.semilogx(Cs,scores)\n",
    "plt.show()\n",
    "\n",
    "# Let's apply the best C parameter found to the test set\n",
    "parval=grid_search.best_params_\n",
    "#knc = LinearSVC(C=parval['C']) \n",
    "knc = SVC(C=parval['C'],kernel='linear')\n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n",
    "print(\"\\nBest value of parameter C found: \",parval)\n",
    "print(\"\\nNumber of supports: \",np.sum(knc.n_support_), \"(\",np.sum(np.abs(knc.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "print(\"Prop. of supports: \",np.sum(knc.n_support_)/X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this dataset, the best C for a linear SVM is 1 (that casually is also the default value for parameter C) so we don't obtain any improvement tuning the C parameter. However, in other datasets we could obtain a dramatic increase of accuracy. \n",
    "\n",
    "Let's see (just for fun) how the C parameter affects performance on training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def plot_validation_curve(parameter_values, train_scores, validation_scores):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, train_scores_mean.max() + .1)\n",
    "    plt.legend(loc=4)\n",
    "\n",
    "\n",
    "training_scores, test_scores = validation_curve(SVC(kernel='linear'), X_train, y_train, param_name=\"C\", param_range=Cs,cv=10)\n",
    "plot_validation_curve(range(len(Cs)), training_scores, test_scores)\n",
    "plt.xticks(range(len(Cs)), Cs,rotation='vertical');\n",
    "plt.ylim([0.6, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that from value of C=1, increasing C results in better accuracy on the training set but worse performance on the test set. This is because being too demanding on the separation of data in the training dataset, we are overfitting to it and we decrease performance in the test set. A nice picture of typical overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernels\n",
    "\n",
    "We have seen that performance with a linear SVM is Ok but not competitive with Metamethods. However, it could happen that using kernels we could even improve accuracy. We'll try first ploynomial kernel with degree 2 with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[843 519 453]\n",
      " [565 947 519]\n",
      " [683 414 708]]\n",
      "\n",
      "Accuracy on test set:  0.442045655636\n"
     ]
    }
   ],
   "source": [
    "knc = SVC(kernel='poly',degree =2) \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better performance than the obtained with a linear SVM... It could even be increased because we didn't tune the C parameter for the polynomial kernel. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-3, 11, num=15, base=10.0)\n",
    "\n",
    "param_grid = {'C': Cs}\n",
    "grid_search = GridSearchCV(SVC(kernel='poly',degree =2) , param_grid, cv=10)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "plt.semilogx(Cs,scores)\n",
    "plt.show()\n",
    "\n",
    "parval=grid_search.best_params_\n",
    "knc = SVC(kernel='poly',degree =2,C=parval['C']) \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n",
    "print(\"\\nBest combination of parameters found: \",parval)\n",
    "print(\"\\nNumber of supports: \",np.sum(knc.n_support_), \"(\",np.sum(np.abs(knc.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "print(\"Prop. of supports: \",np.sum(knc.n_support_)/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result so far. Competitive with meta-methods. Now the best C value found is 100 and accuracy is a lot higher than with default parameters. It's always important when working with SVMs to find best parameters. \n",
    "\n",
    "For this C value we have a nice accuracy on the test set. Let's try what happens now with a polynomial kernel of degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[888 539 388]\n",
      " [591 992 448]\n",
      " [640 435 730]]\n",
      "\n",
      "Accuracy on test set:  0.461865156609\n"
     ]
    }
   ],
   "source": [
    "knc = SVC(kernel='poly',degree =3) \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad results! That's because, again, we didn't use the optimal value for the C parameter but the default one. Let's find the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-3, 11, num=15, base=10.0)\n",
    "\n",
    "param_grid = {'C': Cs}\n",
    "grid_search = GridSearchCV(SVC(kernel='poly',degree =3) , param_grid, cv=10)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "plt.semilogx(Cs,scores)\n",
    "plt.show()\n",
    "\n",
    "parval=grid_search.best_params_\n",
    "knc = SVC(kernel='poly',degree =3,C=parval['C']) \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nBest combination of parameters found: \",parval)\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n",
    "print(\"\\nNumber of supports: \",np.sum(knc.n_support_), \"(\",np.sum(np.abs(knc.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "print(\"Prop. of supports: \",np.sum(knc.n_support_)/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix shows only 1 more error than the confusion matrix obtained with the quadratic kernel. Given the low number of cases we have in the test set, we cannot conclude that polynomial kernel with degree 2 is better than with degree 3. But remember that when there are several classifiers with a similar performance, we should always choose the simpler one! So we will choose the quadratic polynomial kernel as best polynomial kernel, not because of performance only but because is the better combination of performance and simplicity.\n",
    "\n",
    "## RBF Kernel\n",
    "\n",
    "There's another possibility for the kernel: The RBF kernel. This is the default kernel in the implementation of SVMs in sklearn, so we don't need to explicitely say the kernel used. Let's try it with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[843 547 425]\n",
      " [572 980 479]\n",
      " [677 435 693]]\n",
      "\n",
      "Accuracy on test set:  0.445230932578\n"
     ]
    }
   ],
   "source": [
    "knc = SVC() \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promising performance for default parameters. But we have to search for the best parameters. In this case we have two parameters to adjust: the C parameter and the gamma parameter. We will find the best combination using the *GridSearch* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Values we will test for each parameter. When observin results, consider the limits of the \n",
    "# values tested and increase them if necessary \n",
    "gammas = [0.000001,0.00001, 0.0001,0.001,0.01,0.1,1,10]\n",
    "Cs = np.logspace(-1, 6, num=8, base=10.0)\n",
    "\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=10)\n",
    "grid_search.fit(X_train,y_train)\n",
    "parval=grid_search.best_params_\n",
    "\n",
    "# We'll show in a grid, the accuracy for each combination of parameters tester\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "scores = np.array(scores).reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "\n",
    "plt.matshow(scores)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(param_grid['gamma'])), param_grid['gamma'],rotation='vertical')\n",
    "plt.yticks(np.arange(len(param_grid['C'])), param_grid['C'])\n",
    "plt.show()\n",
    "print(\"\\nBest combination of parameters found: \",parval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture show for each combination of parameters the accuracy obtained in a 10-fold cross-validation. Notice the relation between C and gamma. \n",
    "\n",
    "Let's see the performance of the best parameters found on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's apply the best combination of parameters found to the test set\n",
    "parval=grid_search.best_params_\n",
    "knc = SVC(C=parval['C'], gamma=parval['gamma']) \n",
    "knc.fit(X_train, y_train)\n",
    "pred=knc.predict(X_test)\n",
    "\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n",
    "print(\"\\nNumber of supports: \",np.sum(knc.n_support_), \"(\",np.sum(np.abs(knc.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "print(\"Prop. of supports: \",np.sum(knc.n_support_)/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Best performance obtained so far using any method, even better than results obtained with meta-methods. \n",
    "\n",
    "**Observations about number and percentage of supports vectors:** \n",
    "It is know that percentage of supports of an SVM is a lower bound for the leave-one-out error. In general, an SVM with a lot of supports will be an overfitted SVM. A percentage of supports higher than 50% should be considered suspicious. If this happens, try to use other kernels. As a rule of thumb, a good SVM has a percentatge of supports vectors about 20-40% of the data (but that depends on a lot of things).\n",
    "\n",
    "In our case all SVM have a low number of supports. And notice that the machine with a higher performance is the one with a lower number of supports (24.9%). That's not a coincidence but something common in SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Performance of meta-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf=ExtraTreesClassifier(n_estimators=200,random_state=1).fit(X_train, y_train)\n",
    "pred=clf.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf=AdaBoostClassifier(n_estimators=200,random_state=1).fit(X_train, y_train)\n",
    "pred=clf.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf=BaggingClassifier(n_estimators=200,max_features=0.35,random_state=1).fit(X_train, y_train)\n",
    "pred=clf.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy on test set: \",sklearn.metrics.accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
